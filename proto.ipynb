{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from bridges import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-310052\n",
      "WARNING:tensorflow:From /home/jcw/anaconda3/envs/tf14/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-310052\n"
     ]
    }
   ],
   "source": [
    "m = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blah blah auxamer ». (']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.gen('blah blah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lists(ll):\n",
    "    if not isinstance(ll[0],(list, tuple, np.ndarray)):\n",
    "        print(ll)\n",
    "    else:\n",
    "        print(*ll, sep='\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(logits, verbose=False):\n",
    "    mu = np.mean(logits, axis=-1, keepdims=True)\n",
    "    lm = logits - mu\n",
    "    le = np.exp(lm)\n",
    "    logprobs = le / np.sum(le, axis=-1, keepdims=True)\n",
    "    if verbose:\n",
    "        return logprobs, mu, lm, le\n",
    "    return logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs, mu, lm, le = normalize(logits, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.638048"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logits[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.638048], dtype=float32)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm[0,0,0] == logits[0,0,0] - mu[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(lm[0,0,0]) == le[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12, 1)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(le, axis=-1, keepdims=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14673772.], dtype=float32)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(le, axis=-1, keepdims=True)[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs[0,0,0] == le[0,0,0] / np.sum(le, axis=-1, keepdims=True)[0,0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(model, sentences=[\"\\n\"], batch_size=None, verbose=False):\n",
    "    # save current batch_size\n",
    "    batch_size_bak=None\n",
    "    if batch_size is not None and self.batch_size != batch_size:\n",
    "        batch_size_bak = self.batch_size\n",
    "    model.check_batch_size(batch_size)\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    tkns = model.pad_sequences(sentences)\n",
    "    print('sequences:')\n",
    "    print_lists(tkns)\n",
    "        \n",
    "    logits = model.get_logits(context_tokens=tkns)[:, :-1, :] # None to keep dims intact\n",
    "    # don't take the last one (predicting the token after our sentence)\n",
    "    \n",
    "    print(\"logits, shape:\", logits.shape)\n",
    "    \n",
    "    # normalization between 0 and 1\n",
    "    # most importantly: make all \n",
    "    logprobs = normalize(logits)\n",
    "    print(\"logprobs, shape:\", logprobs.shape)\n",
    "    \n",
    "#     return logprobs, tkns\n",
    "#     print(tkns.shape)\n",
    "#     for i, token in enumerate(tkns[:, :-1]):\n",
    "#         print(i, token)\n",
    "\n",
    "#     scores = np.nan_to_num(\n",
    "#         [logprobs[:, i, token] for i, token in enumerate(tkns[:-1])]\n",
    "#     )    \n",
    "    scores = np.nan_to_num(\n",
    "        [\n",
    "            [logprobs[b, i, token] \n",
    "             for b in range(len(tkns))             \n",
    "             for i, token in enumerate(tkns[:, :-1])]            \n",
    "        ]\n",
    "    )\n",
    "    print(\"scores, shape:\", scores.shape)\n",
    "    perplexities = 2 ** (-np.mean(np.log2(scores), axis=-1))\n",
    "#     # restoring batch size \n",
    "#     if batch_size_bak: model.check_batch_size(batch_size_bak)    \n",
    "#     return perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "snt = [\n",
    "    \"Ah, nous voilà bien\",\n",
    "    \"Je me baladais dans le gris sans fond\",\n",
    "    \"LE COMTE. Quel déshonneur !\",\n",
    "    \"Ah.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:\n",
      "[  220   220   220 10910    11   299   516  7608   346 24247   275  2013]\n",
      "[40932   502  3652  4763   271   288   504   443  1036   271 38078 16245]\n",
      "[  220  2538  9440  9328    13  2264   417 39073  1477 47476   333  5145]\n",
      "[  220   220   220   220   220   220   220   220   220   220 10910    13]\n",
      "logits, shape: (4, 11, 50257)\n",
      "logprobs, shape: (4, 11, 50257)\n",
      "[[  220   220   220 10910    11   299   516  7608   346 24247   275]\n",
      " [40932   502  3652  4763   271   288   504   443  1036   271 38078]\n",
      " [  220  2538  9440  9328    13  2264   417 39073  1477 47476   333]\n",
      " [  220   220   220   220   220   220   220   220   220   220 10910]]\n",
      "scores, shape: (1, 16, 11)\n"
     ]
    }
   ],
   "source": [
    "get_perplexity(m, snt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = m.pad_sequences(snt)\n",
    "logits = m.get_logits(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12, 50257)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = normalize(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 12, 50257)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.nan_to_num([[logprobs[j, i, token] \n",
    " for i, token in enumerate(batch[:-1])]\n",
    " for j, batch in enumerate(padded)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = 2 ** (-np.mean(np.log2(scores), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77367.34\n",
      "188906.8\n",
      "2907181.0\n",
      "2.7888317\n"
     ]
    }
   ],
   "source": [
    "print(*perplexities, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.289879 ,  9.88566  , 10.730696 , ...,  5.7731833,  6.458465 ,\n",
       "         5.841981 ],\n",
       "       [ 3.1374109,  1.3310282,  4.4881535, ...,  3.720184 ,  3.8290942,\n",
       "         3.7567344],\n",
       "       [ 3.4757946,  0.8460077,  4.568767 , ...,  3.2985313,  3.3040564,\n",
       "         3.3131304],\n",
       "       ...,\n",
       "       [ 2.0084343, -8.294467 , -6.12812  , ..., -9.5075   , -9.178121 ,\n",
       "        -9.197476 ],\n",
       "       [ 7.3087406,  6.4965568,  6.595199 , ...,  1.4063548,  2.3194256,\n",
       "         1.880352 ],\n",
       "       [ 3.4282572, -8.777307 , -6.4584146, ..., -9.818342 , -9.568864 ,\n",
       "        -9.863892 ]], dtype=float32)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  220],\n",
       "       [  220],\n",
       "       [  220],\n",
       "       [10910],\n",
       "       [   11],\n",
       "       [  299],\n",
       "       [  516],\n",
       "       [ 7608],\n",
       "       [  346],\n",
       "       [24247],\n",
       "       [  275],\n",
       "       [ 2013]], dtype=int32)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [(b, i, tkn) for  in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 220),\n",
       " (1, 220),\n",
       " (2, 220),\n",
       " (3, 10910),\n",
       " (4, 11),\n",
       " (5, 299),\n",
       " (6, 516),\n",
       " (7, 7608),\n",
       " (8, 346),\n",
       " (9, 24247),\n",
       " (10, 275),\n",
       " (11, 2013)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(np.arange(len(padded[0])),padded[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22.68  18.83  18.52  -0.17  -7.89  -0.47  -9.74  -3.58   3.29  -7.50   0.54  -6.90 \n",
      "  5.66   4.03   4.06 -11.92 -11.42   0.89  -4.92  -2.19   0.91  -5.07  -4.36  -2.13 \n",
      " 22.68   1.53   7.14   8.36   0.53   1.24  -1.31  -0.13   0.08 -10.10  -5.05   2.72 \n",
      " 22.68  18.83  18.52  18.08  19.06  20.29  21.18  21.54  21.77  21.78   1.73   0.93 \n"
     ]
    }
   ],
   "source": [
    "for j, (b, tkns) in enumerate(zip(logits, padded)):\n",
    "    for i, tkn in enumerate(tkns):\n",
    "        print(f\"{b[i,tkn]:>6.2f}\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "tokens:\t   220 |    220 |    220 |  10910 |     11 |    299 |    516 |   7608 |    346 |  24247 |    275 |   2013 | \n",
      "logits:\t 22.68 |  18.83 |  18.52 |  -0.17 |  -7.89 |  -0.47 |  -9.74 |  -3.58 |   3.29 |  -7.50 |   0.54 |  -6.90 | \n",
      "batch: 1\n",
      "tokens:\t 40932 |    502 |   3652 |   4763 |    271 |    288 |    504 |    443 |   1036 |    271 |  38078 |  16245 | \n",
      "logits:\t  5.66 |   4.03 |   4.06 | -11.92 | -11.42 |   0.89 |  -4.92 |  -2.19 |   0.91 |  -5.07 |  -4.36 |  -2.13 | \n",
      "batch: 2\n",
      "tokens:\t   220 |   2538 |   9440 |   9328 |     13 |   2264 |    417 |  39073 |   1477 |  47476 |    333 |   5145 | \n",
      "logits:\t 22.68 |   1.53 |   7.14 |   8.36 |   0.53 |   1.24 |  -1.31 |  -0.13 |   0.08 | -10.10 |  -5.05 |   2.72 | \n",
      "batch: 3\n",
      "tokens:\t   220 |    220 |    220 |    220 |    220 |    220 |    220 |    220 |    220 |    220 |  10910 |     13 | \n",
      "logits:\t 22.68 |  18.83 |  18.52 |  18.08 |  19.06 |  20.29 |  21.18 |  21.54 |  21.77 |  21.78 |   1.73 |   0.93 | \n"
     ]
    }
   ],
   "source": [
    "for j, (b, tkns) in enumerate(zip(logits, padded)):\n",
    "    print('batch:', j)\n",
    "    print('tokens:\\t', end='')\n",
    "    for tkn in tkns:\n",
    "        print(f\"{tkn:>6} | \", end='')\n",
    "    print()\n",
    "    print('logits:\\t', end='')\n",
    "    for i, tkn in enumerate(tkns):\n",
    "        print(f\"{b[i,tkn]:>6.2f} |\", end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [  220   220   220 10910    11   299   516  7608   346 24247   275]\n",
      "1 [40932   502  3652  4763   271   288   504   443  1036   271 38078]\n",
      "2 [  220  2538  9440  9328    13  2264   417 39073  1477 47476   333]\n",
      "3 [  220   220   220   220   220   220   220   220   220   220 10910]\n"
     ]
    }
   ],
   "source": [
    "for i, a in enumerate(padded[:, :-1]):\n",
    "    print(i, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(model, context_tokens, last_only=False):\n",
    "    \"\"\"\n",
    "    Generate the logits (probabilities of each token) at each step for a\n",
    "    given one or more sequences of tokens.\n",
    "    Returns:\n",
    "    --------\n",
    "        logits: array of shape: (batch_size, n_tokens, n_vocab)\n",
    "                or, if last_only is True: (batch_size, 1, n_vocab)\n",
    "    \"\"\"\n",
    "    # model.check_batch_size(batch_size)\n",
    "    if not isinstance(context_tokens[0], (list, tuple, np.ndarray)):\n",
    "        model.check_batch_size(1)\n",
    "        context = model.batch_size * [context_tokens]\n",
    "        print('one dim')\n",
    "        print(context)\n",
    "    else:\n",
    "        model.check_batch_size(len(context_tokens))\n",
    "        context = context_tokens\n",
    "        print('many dims')        \n",
    "        print(context)\n",
    "    # model.model, defined at initialization, returns logits & attention matrix\n",
    "    logits = model.sess.run(model.model, feed_dict={model.context: context})[\"logits\"]\n",
    "    # all logits starting from the second token, n logits for n tokens\n",
    "    # shape (batch_size, n_tokens, vocab_size)\n",
    "    if not last_only:\n",
    "        return logits\n",
    "    # logits for next token, None to keep dims intact\n",
    "    return logits[:, None, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one dim\n",
      "[array([10910,    11,   299,   516,  7608,   346, 24247,   275,  2013])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 9, 50257)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(m, m.encode(snt[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many dims\n",
      "[[  220   220   220 10910    11   299   516  7608   346 24247   275  2013]\n",
      " [40932   502  3652  4763   271   288   504   443  1036   271 38078 16245]\n",
      " [  220  2538  9440  9328    13  2264   417 39073  1477 47476   333  5145]\n",
      " [  220   220   220   220   220   220   220   220   220   220 10910    13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 12, 50257)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(m, m.pad_sequences(snt)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 11, 50257)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(m, snt)[:, :-1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 50257)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_logits(m, snt, last_only=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  220,   220,  2436,   993, 33367],\n",
       "       [ 2436,   993,   288,   571,  9271]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pad_sequences(['blah blah', 'blah diblah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  220,   220,   220, 10910,    11,   299,   516,  7608,   346,\n",
       "        24247,   275,  2013],\n",
       "       [40932,   502,  3652,  4763,   271,   288,   504,   443,  1036,\n",
       "          271, 38078, 16245],\n",
       "       [  220,  2538,  9440,  9328,    13,  2264,   417, 39073,  1477,\n",
       "        47476,   333,  5145],\n",
       "       [  220,   220,   220,   220,   220,   220,   220,   220,   220,\n",
       "          220, 10910,    13]], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    m.encode(snt), \n",
    "    maxlen=None, \n",
    "    dtype='int32', \n",
    "    padding='pre', \n",
    "    truncating='pre',\n",
    "    value=220\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([220])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.encode(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having coded the function into the class (220 is space):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  220   220   220 10910    11   299   516  7608   346 24247   275  2013]\n",
      " [40932   502  3652  4763   271   288   504   443  1036   271 38078 16245]\n",
      " [  220  2538  9440  9328    13  2264   417 39073  1477 47476   333  5145]\n",
      " [  220   220   220   220   220   220   220   220   220   220 10910    13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['   Ah, nous voilà bien',\n",
       " 'Je me baladais dans le gris sans fond',\n",
       " ' LE COMTE. Quel déshonneur !',\n",
       " '          Ah.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m.pad_sequences(snt, padding='pre'))\n",
    "m.decode(m.pad_sequences(snt, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10910    11   299   516  7608   346 24247   275  2013   220   220   220]\n",
      " [40932   502  3652  4763   271   288   504   443  1036   271 38078 16245]\n",
      " [ 2538  9440  9328    13  2264   417 39073  1477 47476   333  5145   220]\n",
      " [10910    13   220   220   220   220   220   220   220   220   220   220]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ah, nous voilà bien   ',\n",
       " 'Je me baladais dans le gris sans fond',\n",
       " 'LE COMTE. Quel déshonneur ! ',\n",
       " 'Ah.          ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m.pad_sequences(snt, padding='post'))\n",
    "m.decode(m.pad_sequences(snt, padding='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding and decoding in turn seem to yield identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True  True  True  True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(m.encode(m.decode(m.pad_sequences(snt, padding='post'))) == m.pad_sequences(snt, padding='post'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
